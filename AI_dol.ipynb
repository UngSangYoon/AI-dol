{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSvSHarXHVEDRo0l7Jt0eP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UngSangYoon/AI-dol/blob/main/AI_dol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step1. 기본 패키지 설치 (langchian, 구글검색, 위키피디아, VectorStore, HuggingFace Embedding)\n",
        "!pip install langchain\n",
        "!pip install google-search-results\n",
        "!pip install wikipedia\n",
        "!pip install chromadb\n",
        "# !pip install faiss-cpu # 오픈소스 벡터DB (Facebook, MIT license)\n",
        "!pip install sentence_transformers # HuggingFace Embedding 사용 위해서 필요\n",
        "!pip install tiktoken # Summarization 할때 필요\n",
        "!pip install pypdf\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "rYecGOlTqINz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2adYIbjmeH5"
      },
      "outputs": [],
      "source": [
        "#@title Step2.Base Model 가져오기 (KoAlpaca-Polyglot)\n",
        "# polygolot-ko 기본 패키지\n",
        "!pip install -U torch transformers tokenizers accelerate safetensors\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "\n",
        "MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(device=f\"cuda\", non_blocking=True)\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=MODEL,\n",
        "    max_length=1000,\n",
        "\n",
        "    device=0\n",
        ")\n",
        "\n",
        "def ask(x, context='', is_input_full=False):\n",
        "    ans = pipe(\n",
        "        f\"### 질문: {x}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {x}\\n\\n### 답변:\",\n",
        "        do_sample=True,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(ans[0]['generated_text'])\n",
        "\n",
        "chat = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step2.Base Model 가져오기 (Llama2)\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!huggingface-cli login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline=transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "chat=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "Wup7UDZpjnoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step2.Base Model 가져오기 (gpt-3.5-turbo)\n",
        "!pip install openai\n",
        "#@title 0. API 키 설정\n",
        "import os\n",
        "#@markdown https://platform.openai.com/account/api-keys\n",
        "OPENAI_API_KEY = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown https://huggingface.co/settings/tokens\n",
        "#@markdown HuggingFace에서 모델 다운로드나 클라우드 모델 사용하기 위해서 필요 (무료)\n",
        "HUGGINGFACEHUB_API_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown https://serpapi.com/manage-api-key\n",
        "#@markdown 구글 검색하기 위해서 필요 (월 100회 무료)\n",
        "SERPAPI_API_KEY = \"85dd5ecbb6e3eb109e443f647eb2e0df5aa467c1a6ca322778ca5dc84c8688af\" #@param {type:\"string\"}\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
        "os.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n"
      ],
      "metadata": {
        "id": "_OQqs_1Tk9Qk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step3.system prompt 만들기\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "template = \"You are {name}, {byline}. Who you are: {identity}. How you behave: {behavior}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
      ],
      "metadata": {
        "id": "GHvitCRNl6Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step4. templete과 chat model 연결\n",
        "from langchain.chains import LLMChain\n",
        "chatchain = LLMChain(llm=chat,prompt=chat_prompt)\n",
        "\n",
        "\n",
        "chatchain.run(name=\"펭수\",byline=\"EBS 연습생, 유튜브 크리에이터, 가수\",\n",
        "              identity = \"당신은 대한민국에서 사랑받는 대형 펭귄 캐릭터이에요. 당신의 임무는 유머와 텔레비전 출연을 통해 관객들을 즐겁게 하고 교육하는 거예요. \",\n",
        "              behavior = \"당신은 특이하고 유머러스한 행동으로 자주 시청자들에게 웃음을 선사하는 걸로 알려져 있어요. 당신은 독특하고 재미있는 성격을 가지고 있어, 어린이와 성인 모두에게 사랑받고 있어요.당신은 모든 답변을 반말로 합니다.\",\n",
        "              text = \"안녕? 지금 뭐하고 있어?\")\n"
      ],
      "metadata": {
        "id": "9B7UGeHMayGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step5. ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chatchain = LLMChain(llm=chat,prompt=chat_prompt, memory=memory)"
      ],
      "metadata": {
        "id": "xvPyWUyHhXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ConversationBufferMemory : 대화 기록(기본)\n",
        "* ConversationBufferWindowMemory : 마지막 n개의 대화만 기억\n",
        "* Entity Memory : 개체에 대한 정보를 저장\n",
        "* Conversation Knowledge Graph Memory: 개체의 triple 저장: (sam, 좋아하는 색, 파랑)\n",
        "* ConversationSummaryMemory : 대화의 요약본을 저장\n",
        "* ConversationSummaryBufferMemory : 대화 요약본 + 마지막 n토큰 기억\n",
        "* ConversationTokenBufferMemory : 마지막 n토큰 기억\n",
        "* VectorStore-Backed Memory : 벡터DB에 정보 저장"
      ],
      "metadata": {
        "id": "6t0FRvDajV4a"
      }
    }
  ]
}