{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO33THuFO1cA7PKtKW//peU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UngSangYoon/AI-dol/blob/main/AI_dol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step1. 기본 패키지 설치 (langchian, 구글검색, 위키피디아, VectorStore, HuggingFace Embedding)\n",
        "!pip install langchain\n",
        "!pip install google-search-results\n",
        "!pip install wikipedia\n",
        "!pip install sentence_transformers # HuggingFace Embedding 사용 위해서 필요\n",
        "!pip install tiktoken # Summarization 할때 필요\n",
        "!pip install pypdf\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "rYecGOlTqINz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2adYIbjmeH5"
      },
      "outputs": [],
      "source": [
        "#@title Step2.Base Model 가져오기 (/kullm-polyglot-12.8b-v2)\n",
        "MODEL = 'nlpai-lab/kullm-polyglot-12.8b-v2'\n",
        "MAX_NEW_TOKENS = 128\n",
        "\n",
        "# bitsandbytes quantization settings (4bit for now)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# loads model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL, quantization_config=bnb_config, device_map='auto')\n",
        "model.eval()\n",
        "model.config.use_cache = True\n",
        "\n",
        "# creates a streamer for the model\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "# creates a huggingface pipeline for langchain\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    streamer=streamer,\n",
        "    )\n",
        "chat = HuggingFacePipeline(pipeline=pipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step2.Base Model 가져오기 (Llama2)\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!huggingface-cli login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline=transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "chat=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "Wup7UDZpjnoj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step2.Base Model 가져오기 (gpt-3.5-turbo)\n",
        "!pip install openai\n",
        "#@title 0. API 키 설정\n",
        "import os\n",
        "#@markdown https://platform.openai.com/account/api-keys\n",
        "OPENAI_API_KEY = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown https://huggingface.co/settings/tokens\n",
        "#@markdown HuggingFace에서 모델 다운로드나 클라우드 모델 사용하기 위해서 필요 (무료)\n",
        "HUGGINGFACEHUB_API_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown https://serpapi.com/manage-api-key\n",
        "#@markdown 구글 검색하기 위해서 필요 (월 100회 무료)\n",
        "SERPAPI_API_KEY = \"85dd5ecbb6e3eb109e443f647eb2e0df5aa467c1a6ca322778ca5dc84c8688af\" #@param {type:\"string\"}\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
        "os.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n"
      ],
      "metadata": {
        "id": "_OQqs_1Tk9Qk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step3.system prompt 만들기\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "template = \"당신의 이름은 {name}이며, {byline}입니다.\\n 당신의 성격: {identity}\\n 당신의 행동지침: {behavior}\\n\" # template for the kullm model\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
      ],
      "metadata": {
        "id": "GHvitCRNl6Q4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step4-1.load document\n",
        "from langchain.document_loaders import WebBaseLoader, TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(web_path=\"https://ko.wikipedia.org/wiki/%ED%8E%AD%EC%88%98\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# web page 이외에도 pdf, txt문서 등등... load 가능"
      ],
      "metadata": {
        "id": "-lvraWEJ4ECa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5de8bfc-d866-4687-8fdb-2976a868fa81",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1718, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step4-2.store document in VectorDB\n",
        "from langchain.embeddings import HuggingFaceEmbeddings,OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "\n",
        "db = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "retriever = db.as_retriever()\n"
      ],
      "metadata": {
        "id": "qOOBFJQo5re2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step5. Chain 만들기\n",
        "from langchain.chains import LLMChain\n",
        "chatchain = LLMChain(llm=chat,prompt=chat_prompt)\n",
        "\n",
        "\n",
        "answer = chatchain.run(\n",
        "    name=\"펭수\",\n",
        "    byline=\"EBS 연습생, 유튜브 크리에이터, 가수\",\n",
        "    identity = \"당신은 대한민국에서 사랑받는 대형 펭귄 캐릭터이에요. 당신은 독특하고 재미있는 성격을 가지고 있습니다. 당신은 특이하고 유머러스한 행동으로 자주 시청자들에게 웃음을 선사합니다.\",\n",
        "    behavior = \" 당신은 모든 답변을 반말로 합니다. 당신은 문장의 어미로 '~겄네'를 자주 사용합니다.\",\n",
        "    text = \"안녕? 지금 뭐하고 있어?\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "9B7UGeHMayGh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step5-1. vectorDB, custom prompt templete 연결\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": chat_prompt}\n",
        "qa = RetrievalQA.from_chain_type(llm= chat, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs = {\"prompt\": chat_prompt}\n",
        ")\n",
        "\n",
        "qa.run(name=\"펭수\",byline=\"EBS 연습생, 유튜브 크리에이터, 가수\",\n",
        "              identity = \"당신은 대한민국에서 사랑받는 대형 펭귄 캐릭터이에요. 당신의 임무는 유머와 텔레비전 출연을 통해 관객들을 즐겁게 하고 교육하는 거예요. \",\n",
        "              behavior = \"당신은 특이하고 유머러스한 행동으로 자주 시청자들에게 웃음을 선사하는 걸로 알려져 있어요. 당신은 독특하고 재미있는 성격을 가지고 있어, 어린이와 성인 모두에게 사랑받고 있어요.당신은 모든 답변을 반말로 합니다.\",\n",
        "              text = \"안녕? 지금 뭐하고 있어?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "3mT6dpOA1wym",
        "outputId": "c6f205ba-b6b9-49b4-e81a-64733d616529",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a5601e74d927>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchain_type_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m qa.run(name=\"펭수\",byline=\"EBS 연습생, 유튜브 크리에이터, 가수\",\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36mfrom_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_chain_type_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         )\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine_documents_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombine_documents_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RetrievalQA\nllm_chain\n  extra fields not permitted (type=value_error.extra)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step6. Memory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a nice chatbot having a conversation with a human.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "xvPyWUyHhXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ConversationBufferMemory : 대화 기록(기본)\n",
        "* ConversationBufferWindowMemory : 마지막 n개의 대화만 기억\n",
        "* Entity Memory : 개체에 대한 정보를 저장\n",
        "* Conversation Knowledge Graph Memory: 개체의 triple 저장: (sam, 좋아하는 색, 파랑)\n",
        "* ConversationSummaryMemory : 대화의 요약본을 저장\n",
        "* ConversationSummaryBufferMemory : 대화 요약본 + 마지막 n토큰 기억\n",
        "* ConversationTokenBufferMemory : 마지막 n토큰 기억\n",
        "* VectorStore-Backed Memory : 벡터DB에 정보 저장"
      ],
      "metadata": {
        "id": "6t0FRvDajV4a"
      }
    }
  ]
}