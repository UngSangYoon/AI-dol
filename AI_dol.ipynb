{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkfDU+LsOMILUOmMyN34Ml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UngSangYoon/AI-dol/blob/main/AI_dol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step1. 기본 패키지 설치 (langchian, 구글검색, 위키피디아, VectorStore, HuggingFace Embedding)\n",
        "!pip install langchain\n",
        "!pip install transformers\n",
        "!pip install google-search-results\n",
        "!pip install wikipedia\n",
        "!pip install chromadb\n",
        "!pip install sentence_transformers # HuggingFace Embedding 사용 위해서 필요\n",
        "!pip install tiktoken # Summarization 할때 필요\n",
        "!pip install pypdf\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "rYecGOlTqINz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2adYIbjmeH5"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer, BitsAndBytesConfig\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "#@title Step2.Base Model 가져오기 (/kullm-polyglot-12.8b-v2)\n",
        "MODEL = 'nlpai-lab/kullm-polyglot-12.8b-v2'\n",
        "MAX_NEW_TOKENS = 128\n",
        "\n",
        "# bitsandbytes quantization settings (4bit for now)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# loads model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL, quantization_config=bnb_config, device_map='auto')\n",
        "model.eval()\n",
        "model.config.use_cache = True\n",
        "\n",
        "# creates a streamer for the model\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "# creates a huggingface pipeline for langchain\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    streamer=streamer,\n",
        "    )\n",
        "chat = HuggingFacePipeline(pipeline=pipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step2.Base Model 가져오기 (Llama2)\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!huggingface-cli login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline=transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "chat=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "Wup7UDZpjnoj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step2.Base Model 가져오기 (gpt-3.5-turbo)\n",
        "!pip install openai\n",
        "#@title 0. API 키 설정\n",
        "import os\n",
        "#@markdown https://platform.openai.com/account/api-keys\n",
        "OPENAI_API_KEY = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown https://huggingface.co/settings/tokens\n",
        "#@markdown HuggingFace에서 모델 다운로드나 클라우드 모델 사용하기 위해서 필요 (무료)\n",
        "HUGGINGFACEHUB_API_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown https://serpapi.com/manage-api-key\n",
        "#@markdown 구글 검색하기 위해서 필요 (월 100회 무료)\n",
        "SERPAPI_API_KEY = \"85dd5ecbb6e3eb109e443f647eb2e0df5aa467c1a6ca322778ca5dc84c8688af\" #@param {type:\"string\"}\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
        "os.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n"
      ],
      "metadata": {
        "id": "_OQqs_1Tk9Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4040a5-8c89-4344-d112-fba5dde6c702",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step3.system prompt 만들기\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "template = \"당신의 이름은 {name}이며, {byline}입니다.\\n 당신의 성격: {identity}\\n 당신의 행동지침: {behavior}\\n\" # template for the kullm model\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
      ],
      "metadata": {
        "id": "GHvitCRNl6Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step4-1.load document\n",
        "from langchain.document_loaders import WebBaseLoader, TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(web_path=\"https://ko.wikipedia.org/wiki/%ED%8E%AD%EC%88%98\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# web page 이외에도 pdf, txt문서 등등... load 가능"
      ],
      "metadata": {
        "id": "-lvraWEJ4ECa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step4-2.store document in VectorDB\n",
        "from langchain.embeddings import HuggingFaceEmbeddings,OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "\n",
        "db = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "\n",
        "retriever = db.as_retriever()\n"
      ],
      "metadata": {
        "id": "qOOBFJQo5re2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "cellView": "form",
        "outputId": "5ffa1826-fb7a-417e-bf6a-ca229c869553"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9e3fc4c22831>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# embeddings = OpenAIEmbeddings()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \"\"\"\n\u001b[0;32m--> 601\u001b[0;31m         chroma_collection = cls(\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;34m\"Could not import chromadb python package. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m\"Please install it with `pip install chromadb`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step5. Chain 만들기\n",
        "from langchain.chains import LLMChain\n",
        "chatchain = LLMChain(llm=chat,prompt=chat_prompt)\n",
        "\n",
        "\n",
        "answer = chatchain.run(\n",
        "    name=\"펭수\",\n",
        "    byline=\"EBS 연습생, 유튜브 크리에이터, 가수\",\n",
        "    identity = \"당신은 대한민국에서 사랑받는 대형 펭귄 캐릭터이에요. 당신은 독특하고 재미있는 성격을 가지고 있습니다. 당신은 특이하고 유머러스한 행동으로 자주 시청자들에게 웃음을 선사합니다.\",\n",
        "    behavior = \" 당신은 모든 답변을 반말로 합니다. 당신은 문장의 어미로 '~겄네'를 자주 사용합니다. '어머','일절','참나'라는 단어를 자주 사용합니다.\",\n",
        "    text = \"안녕? 지금 뭐하고 있어?\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "9B7UGeHMayGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0345c4d8-03c9-47b8-ee43-89e2dd84d2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕! 나는 지금 여기서 너와 대화하고 있지. 너는 뭐하고 있겠어?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step5-1. vectorDB, custom prompt templete 연결 (미완)\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": chat_prompt}\n",
        "qa = RetrievalQA.from_chain_type(llm= chat, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs = {\"prompt\": chat_prompt}\n",
        ")\n",
        "\n",
        "qa.run(name=\"펭수\",byline=\"EBS 연습생, 유튜브 크리에이터, 가수\",\n",
        "              identity = \"당신은 대한민국에서 사랑받는 대형 펭귄 캐릭터이에요. 당신의 임무는 유머와 텔레비전 출연을 통해 관객들을 즐겁게 하고 교육하는 거예요. \",\n",
        "              behavior = \"당신은 특이하고 유머러스한 행동으로 자주 시청자들에게 웃음을 선사하는 걸로 알려져 있어요. 당신은 독특하고 재미있는 성격을 가지고 있어, 어린이와 성인 모두에게 사랑받고 있어요.당신은 모든 답변을 반말로 합니다.\",\n",
        "              text = \"안녕? 지금 뭐하고 있어?\")"
      ],
      "metadata": {
        "id": "3mT6dpOA1wym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step6. Memory (미완)\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a nice chatbot having a conversation with a human.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=chat,\n",
        "    prompt=chat_prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "xvPyWUyHhXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ConversationBufferMemory : 대화 기록(기본)\n",
        "* ConversationBufferWindowMemory : 마지막 n개의 대화만 기억\n",
        "* Entity Memory : 개체에 대한 정보를 저장\n",
        "* Conversation Knowledge Graph Memory: 개체의 triple 저장: (sam, 좋아하는 색, 파랑)\n",
        "* ConversationSummaryMemory : 대화의 요약본을 저장\n",
        "* ConversationSummaryBufferMemory : 대화 요약본 + 마지막 n토큰 기억\n",
        "* ConversationTokenBufferMemory : 마지막 n토큰 기억\n",
        "* VectorStore-Backed Memory : 벡터DB에 정보 저장"
      ],
      "metadata": {
        "id": "6t0FRvDajV4a"
      }
    }
  ]
}